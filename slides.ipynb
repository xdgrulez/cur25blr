{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafi: Kafka Superpowers for Your Jupyter Notebook and Python\n",
    "<img src=\"pix/kafka.jpg\" style=\"width: 30%; height: 30%\"/>\n",
    "<img src=\"pix/jupyter.jpg\" style=\"width: 30%; height: 30%\"/>\n",
    "\n",
    "### Ralph Debusmann\n",
    "##### `ralph.debusmann@mgb.ch`\n",
    "\n",
    "<img src=\"pix/migros.png\" style=\"width: 20%; height: 20%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "* Part I: The Birth of Kafi\n",
    "\n",
    "* Part II: Three Paradigms for Using Kafi\n",
    "  * Shell/Python interpreter\n",
    "  * Juypter Notebooks\n",
    "  * Code (Microservices, FaaS, Agents...)\n",
    "\n",
    "* Part III: Use Cases for Kafi\n",
    "  * Kafka Administration\n",
    "  * Schema Registry Administration\n",
    "  * Kafka Backups incl. Kafka Emulation\n",
    "  * Simple Stream Processing\n",
    "  * Kafka via REST Proxy\n",
    "  * Building a Bridge from Kafka to Pandas Dataframes and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: The Birth of Kafi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pix/birth.jpg\" style=\"width: 35%; height: 35%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you would just like to create a topic on Kafka, list topics, produce some messages, or consume some messages, or search for messages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is often:\n",
    "* kafkacat/kcat\n",
    "* standard Kafka commandline tools (kafka-console-producer, kafka-console-consumer...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works...for a long time indeed. But how?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still the State-of-the-Art Developer Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kcat -b localhost:9092 -L\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kafka-topics --bootstrap-server localhost:9092 --list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(not possible with kcat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kafka-topics --bootstrap-server localhost:9092 --topic topic_json1 --create\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kcat -b localhost:9092 -t topic_json1 -P -K ,\n",
    "\n",
    "123,{\"bla\":123}\n",
    "456,{\"bla\":456}\n",
    "789,{\"bla\":789}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kafka-console-producer --bootstrap-server localhost:9092 --topic topic_json1 --property parse.key=true --property key.separator=','\n",
    "\n",
    "123,{\"bla\":123}\n",
    "456,{\"bla\":456}\n",
    "789,{\"bla\":789}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Messages Using a Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(not even possible with kcat...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kafka-avro-console-producer --bootstrap-server localhost:9092 --topic topic_avro1 --property schema.registry.url=http://localhost:8081 --property key.serializer=org.apache.kafka.common.serialization.StringSerializer --property value.serializer=io.confluent.kafka.serializers.KafkaAvroSerializer --property value.schema='{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"bla\",\"type\":\"int\"}]}' --property parse.key=true --property key.separator=','\n",
    "\n",
    "123,{\"bla\": 123}\n",
    "456,{\"bla\": 456}\n",
    "789,{\"bla\": 789}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kcat -b localhost:9092 -t topic_json1 -C -o beginning\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kafka-console-consumer --bootstrap-server localhost:9092 --topic topic_json1 --property print.key=true --from-beginning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kcat -b localhost:9092 -t topic_json1 -C -o beginning -e | grep 456\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "kafka-console-consumer --bootstrap-server localhost:9092 --topic topic_json1 --from-beginning | grep 456\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can't We Do Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I developed Kafi because I was frustrated with kcat and the standard Kafka commandline tools. Not by another commandline tool, but by building a Python module (=library) wrapped around Confluent's Python client for Kafka, confluent_kafka.\n",
    "\n",
    "Regardless of whether you use Kafi in your shell or in a Jupyter notebook, you have a similar experience. And your life gets so much better. I promise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you can list topics, create topics, produce messages, consume messages or search for messages with Kafi.\n",
    "\n",
    "Because Kafi is a Python module, you first need to import it. Then, you create a Cluster object `c` reading from a configuration file:\n",
    "\n",
    "```\n",
    "from kafi.kafi import *\n",
    "c = Cluster(\"local\")\n",
    "```\n",
    "\n",
    "Then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c.ls()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many commands also support wildcards, so like in a shell, you can do e.g.:\n",
    "```\n",
    "c.ls(\"*off*\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c.touch(\"topic_json2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "pr = c.producer(\"topic_json2\")\n",
    "pr.produce({\"bla\": 123}, key=\"123\")\n",
    "pr.produce({\"bla\": 456}, key=\"456\")\n",
    "pr.produce({\"bla\": 789}, key=\"789\")\n",
    "pr.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Messages Using a Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "t = \"topic_avro2\"\n",
    "s = '{\"type\":\"record\",\"name\":\"myrecord\",\"fields\":[{\"name\":\"bla\",\"type\":\"int\"}]}'\n",
    "\n",
    "pr = c.producer(t, value_type=\"avro\", value_schema=s)\n",
    "pr.produce({\"bla\": 123}, key=\"123\")\n",
    "pr.produce({\"bla\": 456}, key=\"456\")\n",
    "pr.produce({\"bla\": 789}, key=\"789\")\n",
    "pr.close()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "c.cat(\"topic_json2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or...\n",
    "\n",
    "```\n",
    "c.cat(\"topic_avro2\")\n",
    "```\n",
    "\n",
    "ok, it's Avro...\n",
    "\n",
    "```\n",
    "c.cat(\"topic_avro2\", value_type=\"avro\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafi supports the full range of configuration options of Confluent's Python client. This is, for example, the simple configuration file to connect to a local Kafka cluster that we used in our first steps with Kafi before:\n",
    "\n",
    "```\n",
    "kafka:\n",
    "  bootstrap.servers: localhost:9092\n",
    "\n",
    "schema_registry:\n",
    "  schema.registry.url: http://localhost:8081\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is a configuration file for connecting to Confluent Cloud:\n",
    "\n",
    "```\n",
    "kafka:\n",
    "  bootstrap.servers: ${KAFI_KAFKA_SERVER}\n",
    "  security.protocol: SASL_SSL\n",
    "  sasl.mechanisms: PLAIN\n",
    "  sasl.username: ${KAFI_KAFKA_USERNAME}\n",
    "  sasl.password: ${KAFI_KAFKA_PASSWORD}\n",
    "  \n",
    "schema_registry:\n",
    "  schema.registry.url: ${KAFI_SCHEMA_REGISTRY_URL}\n",
    "  basic.auth.credentials.source: USER_INFO\n",
    "  basic.auth.user.info: ${KAFI_SCHEMA_REGISTRY_USER_INFO}\n",
    "```\n",
    "\n",
    "There are many other configuration options to fine-tune your cluster connection and to override Kafi's \"common sense\" defaults (e.g. setting the `auto.offset.reset` to `earliest`). These defaults are one of the building blocks responsible for making it so convenient to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Three Paradigms for Using Kafi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pix/paradigms.jpg\" style=\"width: 35%; height: 35%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, this talk is titled \"Kafka Superpowers for Your Jupyter Notebook and Python\". So where is Kafi in the Jupyter notebook? Ok, here, but that's not what you probably ask yourselves... so far, we just used in the Python interpreter in the shell...\n",
    "\n",
    "There are actually three main paradigms for using Kafi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shell/Python Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is in your shell using the Python interpreter, like we did in Part I above. That gives you a user/developer experience similar to bash/zsh + kcat or the standard Kafka commandline tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code (Microservices, FaaS, Agents...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Kafi is just a Python module, it is also super useful to use in your Python code. Either for smaller scripts, or even for building microservices, FaaS-functions, or agents (put in a pinch of llamaindex agents for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally to them. You will see soon in Part III that Jupyter notebooks are a very convenient and powerful paradigm of using Kafi, especially for Python/Jupyter afficionados like Data Scientists etc.\n",
    "\n",
    "But... using Kafi in a Jupyter notebook is actually also very convenient and powerful for Kafka administrators or developers! You'll see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Use Cases for Kafi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pix/use_cases.jpg\" style=\"width: 35%; height: 35%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We covered a bit of that already when we compared Kafi to kcat/the standard Kafka commandline tools. So let's start again by importing Kafi and connecting to our local Kafka cluster, and a Confluent Cloud cluster for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafi.kafi import *\n",
    "cl = Cluster(\"local\")\n",
    "cc = Cluster(\"ccloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brokers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic administration task is to show the brokers of your Kafka cluster. So let's view the brokers of our local Kafka cluster first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.brokers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, now for our Confluent Cloud Basic Cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.brokers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the broker configs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.broker_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Confluent Cloud cluster, we only want to see the config of one broker, broker 11:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.broker_config(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we'd just like to see one configuration item, e.g. `message.max.bytes`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_[11][\"message.max.bytes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can just as well change this configuration item, or at least try to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.broker_config(config={\"message.max.bytes\": 1048582})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well there we are. But it should work on our local cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.broker_config(config={\"message.max.size\": 1048582})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see the consumer groups that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.gls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the automatically created groups that our `cat` command created before. What are the offsets of one of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = \"...\"\n",
    "\n",
    "cl.group_offsets(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we need the same consumer group offsets for another consumer group... on Confluent Cloud? For that, let's first create a topic on Confluent Cloud and populate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"topic_json2\"\n",
    "\n",
    "cc.touch(t)\n",
    "pr = cc.producer(t)\n",
    "pr.produce({\"bla\": 123}, key=\"123\")\n",
    "pr.produce({\"bla\": 456}, key=\"456\")\n",
    "pr.produce({\"bla\": 789}, key=\"789\")\n",
    "pr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go. Copy the offsets of our consumer group on our local cluster to a new one on Confluent Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.cp_group_offsets(t, g, cc, g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.group_offsets(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's close by going back to our local cluster and deleting the source consumer groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.grm(\"17*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.gls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the brokers, we can have a look at the configuration of a topic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"topic_json2\"\n",
    "\n",
    "cl.config(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...we can change the configuation just as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.config(t, {\"retention.ms\": -1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create or delete topics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.touch(\"abc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.rm(\"a*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list topics with their total sizes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.l()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...see their partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.partitions(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and their watermarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.watermarks(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a new test topic and write some messages to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t = \"topic_offsets\"\n",
    "\n",
    "pr = cl.producer(t)\n",
    "pr.produce({\"bla\": 123}, key=\"123\")\n",
    "time.sleep(0.1)\n",
    "pr.produce({\"bla\": 456}, key=\"456\")\n",
    "time.sleep(0.1)\n",
    "pr.produce({\"bla\": 789}, key=\"789\")\n",
    "pr.close()\n",
    "\n",
    "cl.cat(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick the timestamp of the second message and search for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.offsets_for_times(t, {0: ...})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Of course, we can also delete some records from the beginning of the topic. E.g. the first two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl.watermarks(t))\n",
    "\n",
    "cl.delete_records({t: {0: 2}})\n",
    "\n",
    "cl.watermarks(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are left with only the third message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.cat(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also repeat the last message, e.g. useful for testing consumers without having to reset their consumer group offsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.repeat(t, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.cat(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or recreate a topic with the exact same configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.recreate(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.l(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about some statistics about one of our still populated topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"topic_json2\"\n",
    "\n",
    "cl.message_size_stats(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stay with me... we have some more functionality that goes beyond just having a \"cat\" command... all shell-inspired..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See n messages from the beginning of the topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.head(t, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or from the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.tail(t, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it increasingly gets wilder. How about copying a topic from our local cluster to Confluent Cloud?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = t + \"_from_local_cluster\"\n",
    "\n",
    "cc.touch(t2)\n",
    "\n",
    "cl.cp(t, cc, t2)\n",
    "\n",
    "cc.l(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this on Confluent Cloud..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, some more shelly stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.wc(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a diff of the topic on our local cluster with that on Confluent Cloud (should be the same):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.diff(t, cc, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a grep on the topic to find the message with value `456`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.grep(t, \".*456.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and do a grep with our own lambda function instead of a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.grep_fun(t, lambda x: x[\"key\"] == \"789\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Registry Administration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Kafi, you also have the entire array of the Schema Registry API at your disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.sls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or maybe just those matching a pattern (well, in this case we have just one, but it's nice nonetheless)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.sls(\"*-value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the versions of the subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"topic_avro2-value\"\n",
    "\n",
    "cl.get_versions(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the latest version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.get_latest_version(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next. We list the subjects, delete our schema, and list the subjects again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cl.sls())\n",
    "\n",
    "cl.srm(s)\n",
    "\n",
    "cl.sls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see if it is only soft-deleted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.sls(deleted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, so let's kill it off completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.srm(s, permanent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it should really be gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.sls(deleted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the Schema Registry API is also supported:\n",
    "\n",
    "* get_schema\n",
    "* register_schema\n",
    "* lookup_schema\n",
    "* get_schema_versions\n",
    "* get_versions\n",
    "* delete_version\n",
    "* get_compatibility\n",
    "* set_compatibility\n",
    "* test_compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for the first use case - for doing Kafka administration with Kafi :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafi also offers some functionality for simple stream processing. It's nowhere as expressive and powerful as e.g. Kafka Streams or Flink, or other Python libraries like Quix, Bytewax, Pathway etc. - but for many day-to-day tasks and microservices, this could even be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, and shameless plug. If you wish to read up on stream processing and streaming databases, and the ongoing convergence of streaming and databases/data warehouses/data lakes (e.g. Tableflow) - there is a book that I can recommend ;-)\n",
    "\n",
    "<img src=\"pix/sdb.jpg\" style=\"width: 30%; height: 30%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the topic. Kafi and stream processing. All the functionality for stream processing (and actually, even simpler commands like `cat` or `head`) are based on a functional backbone. As a functional programmer, or a Kafka Streams DSL or Flink DataStream API user, you'll feel at home immediately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with `foreach`. Here, we simply read the topic message-by-message and print out its key. We could do anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"topic_json2\"\n",
    "\n",
    "pr = cl.producer(t)\n",
    "pr.produce({\"bla\": 123}, key=\"123\")\n",
    "pr.produce({\"bla\": 456}, key=\"456\")\n",
    "pr.produce({\"bla\": 789}, key=\"789\")\n",
    "pr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"topic_json2\"\n",
    "\n",
    "cl.foreach(t, lambda x: print(x[\"key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we go a bit further and use a `map` function that reads individual messages from a topic, does a \"single message transform\", and returns the result of the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x):\n",
    "    x[\"value\"][\"bla\"] += 1000\n",
    "    return x\n",
    "\n",
    "cl.map(t, add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we can also write out the result of the transformation to another topic, even on another cluster. So let's do the same transformation as above and write the result out to our Confluent Cloud cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = \"topic_json_map\"\n",
    "\n",
    "cc.touch(t2)\n",
    "\n",
    "cl.map_to(t, cc, t2, add)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if that has worked..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next command: `flatmap`. Take individual messages and return a list of them (possibly empty of course). In the following example, we just duplicate the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dup(x):\n",
    "    return [x, x]\n",
    "\n",
    "cl.flatmap(t, dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's write out the result to Confluent Cloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = \"topic_flatmap\"\n",
    "\n",
    "cc.touch(t3)\n",
    "\n",
    "cl.flatmap_to(t, cc, t3, dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check out the result there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next command, `filter`, is just a special case of `flatmap`. Here, we just want to keep the message with value `456`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.filter(t, lambda x: x[\"value\"][\"bla\"] == 456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, `filter_to` is also there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = \"topic_filter\"\n",
    "\n",
    "cc.touch(t4)\n",
    "\n",
    "cl.filter_to(t, cc, t4, lambda x: x[\"value\"][\"bla\"] == 456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more check on Confluent Cloud..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and continue. `foldl` stands for \"fold left\" in functional programming, and is often also called `reduce` (e.g. in Kafka Streams). It is useful for simple stateful stream processing.\n",
    "\n",
    "In the example below, we do a very simple aggregation: We sum up the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(acc, x):\n",
    "    acc += x[\"value\"][\"bla\"]\n",
    "    return acc\n",
    "\n",
    "cl.foldl(t, sum, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, Kafi allows you to write out the result of your processing into another topic. On any cluster. It's a bit more involved though. What we do below is to get the value of each message in the source topic on our local cluster, remove the `bla` field, and add another field `sum` with the current sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = \"topic_foldl\"\n",
    "\n",
    "cc.touch(t5)\n",
    "\n",
    "def sum_to(acc, x):\n",
    "    acc += x[\"value\"][\"bla\"]\n",
    "    #\n",
    "    y = dict(x)\n",
    "    del y[\"value\"][\"bla\"]\n",
    "    y[\"value\"][\"sum\"] = acc\n",
    "    #\n",
    "    return acc, [y]\n",
    "\n",
    "cl.foldl_to(t, cc, t5, sum_to, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now for the last simple stream processing function.\n",
    "\n",
    "We join the source topic with the `bla` field from our local Kafka cluster with the new topic with only the `sum` field on Confluent Cloud, and write out the result to our local Kafka cluster to have a topic that has both fields.\n",
    "\n",
    "We use the key of the messages to join them, as e.g. in Kafka Streams (of course, you can override this and also e.g. use a field in the value payload).\n",
    "\n",
    "BTW the join code is inspired by DBSP/Feldera, if you don't know it, have a look at e.g. this super cool blog on their web page:\n",
    "https://www.feldera.com/blog/gpu-stream-dbsp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t6 = \"topic_join\"\n",
    "\n",
    "cl.join_to(t, cc, t5, cl, t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.cat(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka via REST Proxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire functionality of Kafi cannot only be used via the direct Kafka protocol, but also via a REST Proxy. This might sometimes be necessary if you have a firewall blocking the Kafka port, or a Private Cluster that you can only access via IP whitelisting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it work? You just create a `RestProxy` object instead of `Cluster`, and then e.g. do a `ls`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = RestProxy(\"local\")\n",
    "rl.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is this really going over HTTP? Have a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.verbose(2)\n",
    "rl.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really all the commands that you have seen above also work via REST now. E.g. you can produce to a topic as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"topic_rest_proxy\"\n",
    "\n",
    "pr = rl.producer(t)\n",
    "pr.produce({\"bla\": 123}, key=\"123\")\n",
    "pr.produce({\"bla\": 456}, key=\"456\")\n",
    "pr.produce({\"bla\": 789}, key=\"789\")\n",
    "pr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do a `cat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl.cat(t, key_type=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or do some wild thing like reading a topic via REST and copying it over to Confluent Cloud via the direct Kafka protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trp = \"topic_from_rest_proxy\"\n",
    "\n",
    "cc.touch(trp)\n",
    "\n",
    "rl.cp(t, cc, trp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to that funky UI one more time...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Backups incl. Kafka Emulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kafi has built-in \"Kafka Emulation\". That is e.g. extremely useful e.g. for backing up topics to local disk, and replaying them back 1:1 back to Kafka."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a `Local` object that points to our local hard disk, and backup a topic from Confluent Cloud to it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = Cluster(\"ccloud\") \n",
    "l = Local(\"local\")\n",
    "\n",
    "t = \"topic_json2\"\n",
    "\n",
    "cc.cp(t, l, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out whether the topic has landed on our \"Kafka Emulation\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.l()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. Let's see it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.cat(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, since we have read it, there should be an \"emulated\" consumer group as well, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.gls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.describe_groups()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this look like under the covers? Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Kafi doesn't only support local disk here. You can just as well use Kafi's direct Azure Blob Storage support, or, as we will show, S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = S3(\"local\")\n",
    "s.ls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still empty. So let's copy the topic from somewhere (e.g. our local Kafka cluster) to S3 (local MinIO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.cp(t, s, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.l()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more `cat`, now reading from S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.cat(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this out in the MinIO UI as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and lastly, let's copy back the topic from local S3 \"Kafka Emulation\" to Confluent Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t7 = \"topic_json_from_s3\"\n",
    "\n",
    "cc.touch(t7)\n",
    "\n",
    "s.cp(t, cc, t7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Bridge from Kafka to Pandas Dataframes and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are at the end, oh no, one more thing.\n",
    "\n",
    "Kafi's name doesn't only mean \"coffee\" in Swiss German, but it actually means \"*Ka*fka\" and \"*Fi*les\".\n",
    "\n",
    "In this sense, you can not only use Kafi for doing backups and play them back to Kafka.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Kafi can do, is e.g. copy a topic into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cl.topic_to_df(t)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And back to Kafka..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = \"topic_df\"\n",
    "\n",
    "cl.df_to_topic(df, tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that out... the keys should be missing as these commands yet only use the value of the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.cat(tdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have `topic_to_df` and `df_to_topic`, it is not a far step to use all kinds of file formats supported by Pandas:\n",
    "* csv\n",
    "* feather\n",
    "* json\n",
    "* orc\n",
    "* parquet\n",
    "* xslx\n",
    "* xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence Kafi also supports direct dumping of a topic to a Parquet file for instance, in this example, from the local Kafka cluster to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = S3(\"local\")\n",
    "\n",
    "cl.topic_to_file(t, s, \"topic.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check this out in the MinIO UI..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also copy the topic from e.g. from our local Kafka to an Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.topic_to_file(t, s, \"topic.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and have a look..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, cool thing is, we can go the other way round, too. We can bring back the Excel file on S3 and write it out to a topic on Confluent Cloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texcel = \"topic_from_excel\"\n",
    "\n",
    "cc.touch(texcel)\n",
    "\n",
    "s.file_to_topic(\"topic.xlsx\", cc, texcel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final look into the Confluent Cloud UI hopefully shows us that it has worked..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just head over to GitHub for the Kafi project and its documentation:\n",
    "https://github.com/xdgrulez/kafi\n",
    "\n",
    "This Jupyter notebook can also be found there, in case you'd like to go through it yourself:\n",
    "https://github.com/xdgrulez/cur25blr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pix/thank_you.jpg\" style=\"width: 60%; height: 60%\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
